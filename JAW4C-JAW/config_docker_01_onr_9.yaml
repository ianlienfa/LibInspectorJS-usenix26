# This config is the default config used in the docker compose environment
testbed:
  ## option 1: test a specifc website
  # site: http://240.240.240.240/?target=https%3A%2F%2Fwww.google.com.pr%2Fimghp%3Fhl%3Den%26ogbl&type=soak
  ## option 2: provide a top-site list (e.g., Alexa, Tranco, etc)
  # sitelist: /input/tranco_Z2QWG_unique.csv
  archive:
    enable: true
    mappinglist: /JAW4C/JAW4C-WebArchive/archive/name_mapping.json
  from_row: 6045
  to_row: end # row number or 'end'


# 2. crawler configuration
crawler:
  # max number of urls to visit
  maxurls: 5
  # time budget for crawling each site in seconds
  sitetimeout: 180 # 5 min (since we work on archive)
  # amount of memory for the crawler
  memory: 8192
  # overwrite already existing crawled data or not
  overwrite: true
  # check if domain is up with a python request before spawning a browser instance
  domain_health_check: false
  # puppeteer additional args
  puppeteer:
    load-extension: /JAW4C/JAW4C-PTV
    load-extension-original: /JAW4C/JAW4C-PTV-Original
    proxy-server: http://proxy:8002
    ignore-certificate-errors: true
    disk-cache-dir: /dev/null
    disk-cache-size: 1

  playwright:
    proxy:
      server: http://proxy:8002


  # browser to use for crawling
  browser:
    name: firefox # options are `chrome` (crawler.js) and `firefox` (crawler-taint.js)
    headless: true
    # use foxhound if firefox is enabled (default is true)
    foxhound: true
    foxhoundpath: /JAW4C/JAW4C-JAW/crawler/foxhound

  lib_detection:
    enable: true
    detection_timeout: 60 # 60
    post_cleanup: true  # cleanup lifted file or not
    detector:
      load-extension: /JAW4C/JAW4C-PTV
      load-extension-original: /JAW4C/JAW4C-PTV-Original
      proxy-server: http://proxy:8002

vuln_db:
  connect: true
  host: db
  port: 5432
  dbname: vulndb_annotated
  user: vulndb_annotated
  password: vulndb_pwd

# 3. static analysis configuration
staticpass:
  # time budget for static analysis of each site (in seconds)
  sitetimeout: 10800 # 3 hrs
  # enforce a max per webpage timeout when `sitetimeout` is not used (in seconds)
  pagetimeout: 3600 # 1 hr
  # iteratively write the graph output to disk (useful in case of timeouts for partial results)
  iterativeoutput: false
  # max amount of available memory for static analysis per process
  memory: 32000
  # compress the property graph or not
  compress_hpg: true
  # overwrite the existing graphs or not
  overwrite_hpg: true
  # timeout for neo4j container transactions in seconds (default: 300)
  container_transaction_timeout: 300
  # neo4j instance config
  neo4j_user: neo4j
  neo4j_pass: root
  neo4j_http_port: '7482'
  # bolt port will default to http port + 2 with ineo
  # otherwise, specify another port here
  neo4j_bolt_port: '7695'
  neo4j_use_docker: true
  keep_docker_alive: false
  # Maximum number of matching nodes to process per code pattern (prevents performance issues)
  code_matching_cutoff: 50
  # Maximum recursion depth for taint propagation (prevents stack overflow)
  call_count_limit: 5

# 4. dynamic analysis configuration
dynamicpass:
  # time budget for dynamic analysis of each site in seconds
  sitetimeout: 10800 # 3 hrs
  # which browser to use
  browser:
    name: chrome
    # use remote browserstack browsers or not
    use_browserstack: false
    browserstack_username: xyz
    browserstack_password: xyz
    browserstack_access_key: xyz

# 5. verification pass
verificationpass:
  sitetimeout: 10800 # 3 hrs
  # which browser to use
  browser:
    name: chrome
  endpoint: http://127.0.0.1:3456

cve_vuln:
  enabled: true
  passes:
    crawling: true #true
    lib_detection: true #true
    vulndb: true
    static: true
    static_neo4j: true
    lift: true #true  # enable lifting functionality
    transform: true #true  # enable sequence expression transformation
    # verification: false
    # crawling: false
    # static: true
    # static_neo4j: true
