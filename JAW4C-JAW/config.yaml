# 1. which webapps to test?
# to enable archive, remember to comment the 'site' tag
testbed: 
  ## option 1: test a specifc website
  # site: http://240.240.240.240/?target%3Dhttps%3A%2F%2Fwww.nukistream.com%2Fcategory.php%3Fid%3D13%26type%3Dsoak
  ## option 2: provide a top-site list (e.g., Alexa, Tranco, etc)
  # sitelist: /input/tranco_Z2QWG_unique.csv
  archive: 
    enable: true
    mappinglist: /home/ian/JAW4C/JAW4C-WebArchive/archive/archive-70/name_mapping.json
  from_row: 240
  to_row: end # row number or 'end'


# 2. crawler configuration
crawler:
  # max number of urls to visit
  maxurls: 5
  # time budget for crawling each site in seconds
  sitetimeout: 180 # 5 min (since we work on archive)
  # amount of memory for the crawler
  memory: 8192
  # overwrite already existing crawled data or not
  overwrite: true 
  # check if domain is up with a python request before spawning a browser instance
  domain_health_check: false
  # puppeteer additional args
  puppeteer:    
    disable-extensions-except: /home/ian/JAW4C/JAW4C-PTV
    load-extension: /home/ian/JAW4C/JAW4C-PTV
    proxy-server: 127.0.0.1:8002
    ignore-certificate-errors: true
    disk-cache-dir: /dev/null
    disk-cache-size: 1

  playwright:   
    executablePath: /usr/bin/chromium-browser # default path          
    proxy: 
      server: http://localhost:8002


  # browser to use for crawling
  browser:
    name: firefox # options are `chrome` (crawler.js) and `firefox` (crawler-taint.js)
    headless: true
    # use foxhound if firefox is enabled (default is true)
    foxhound: true
    foxhoundpath: /home/ian/project-foxhound/obj-tf-release/dist/bin/foxhound
    
  lib_detection:
    enable: true
    
    detection_timeout: 60 # 60
    post_cleanup: true  # cleanup lifted file or not    

vuln_db:
  connect: true 
  host: localhost
  port: 5430
  dbname: vulndb_annotated
  user: vulndb_annotated
  password: vulndb_pwd

# 3. static analysis configuration
staticpass:
  # time budget for static analysis of each site (in seconds)
  sitetimeout: 10800 # 3 hrs
  # enforce a max per webpage timeout when `sitetimeout` is not used (in seconds)
  pagetimeout: 900 # 15min
  # iteratively write the graph output to disk (useful in case of timeouts for partial results)
  iterativeoutput: false
  # max amount of available memory for static analysis per process
  memory: 32000
  # compress the property graph or not
  compress_hpg: true
  # overwrite the existing graphs or not
  overwrite_hpg: true
  # timeout for neo4j container transactions in seconds (default: 300)
  container_transaction_timeout: 300
  # neo4j instance config
  neo4j_user: neo4j
  neo4j_pass: root
  neo4j_http_port: '7474'
  # bolt port will default to http port + 2 with ineo
  # otherwise, specify another port here
  neo4j_bolt_port: '7476'
  neo4j_use_docker: true
  keep_docker_alive: false

# 4. dynamic analysis configuration
dynamicpass:
  # time budget for dynamic analysis of each site in seconds
  sitetimeout: 10800 # 3 hrs
  # which browser to use
  browser:
    name: chrome
    # use remote browserstack browsers or not
    use_browserstack: false
    browserstack_username: xyz
    browserstack_password: xyz
    browserstack_access_key: xyz

# 5. verification pass
verificationpass:
  sitetimeout: 10800 # 3 hrs
  # which browser to use
  browser:
    name: chrome
  endpoint: http://127.0.0.1:3456

cve_vuln:
  enabled: true
  passes:
    crawling: true
    lib_detection: true  
    vulndb: true
    static: true
    static_neo4j: true  
    lift: true  # enable lifting functionality
    transform: true  # enable sequence expression transformation
    # verification: false
    # crawling: false
    # static: true
    # static_neo4j: true
