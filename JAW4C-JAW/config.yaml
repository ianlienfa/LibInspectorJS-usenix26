# 1. which webapps to test?
# to enable archive, remember to comment the 'site' tag
testbed: 
  ## option 1: test a specifc website
  # site: http://240.240.240.240/?target=https%3A%2F%2Fwww.ebay.de%2Fsch%2Fi.html%3F_from%3DR40%26_nkw%3D%2520%26_sacat%3D131090%26LH_TitleDesc%3D0%26_fspt%3D1%26LH_PrefLoc%3D99%26_sadis%3D25%26LH_LPickup%3D1&type=soak
  ## option 2: provide a top-site list (e.g., Alexa, Tranco, etc)
  # sitelist: /input/tranco_Z2QWG_unique.csv
  archive: 
    enable: true
    # mappinglist: /home/ian/JAW4C/JAW4C-WebArchive/archive/archive_2026_jan5_rank0_18100_norep_0/name_mapping.json
    mappinglist: /home/ian/JAW4C/JAW4C-WebArchive/archive/archive-70/name_mapping.json    
    # mappinglist: /home/ian/JAW4C/JAW4C-WebArchive/archive/archive-dec24/name_mapping.json
    # mappinglist: /home/ian/JAW4C/JAW4C-WebArchive/archive/archive_2026_0101_run2/name_mapping.json
  # Option A: Continuous range (from_row to to_row)
  from_row: 2544  #2936 the debugging site
  to_row: 2544 # 2936 # row number or 'end'

  # Option B: Non-continuous indices (takes precedence over from_row/to_row if uncommented)
  # Specify a list of specific row indices to process
  # row_indices: [2981, 2960, 2952, 2944, 2936, 2935, 2928, 2898]

  # site_list: /home/ian/JAW4C/JAW4C-JAW/vuln_urls.json


# 2. crawler configuration
crawler:
  # max number of urls to visit
  maxurls: 5
  # time budget for crawling each site in seconds
  sitetimeout: 180 # 5 min (since we work on archive)
  # amount of memory for the crawler
  memory: 8192
  # overwrite already existing crawled data or not
  overwrite: true 
  # check if domain is up with a python request before spawning a browser instance
  domain_health_check: false
  # puppeteer additional args
  puppeteer:    
    disable-extensions-except: /home/ian/JAW4C/JAW4C-PTV
    load-extension: /home/ian/JAW4C/JAW4C-PTV
    proxy-server: 127.0.0.1:8002
    ignore-certificate-errors: true
    disk-cache-dir: /dev/null
    disk-cache-size: 1

  playwright:   
    executablePath: /usr/bin/chromium-browser # default path          
    proxy: 
      server: http://localhost:8002


  # browser to use for crawling
  browser:
    name: firefox # options are `chrome` (crawler.js) and `firefox` (crawler-taint.js)
    headless: true
    # use foxhound if firefox is enabled (default is true)
    foxhound: true
    foxhoundpath: /home/ian/project-foxhound/obj-tf-release/dist/bin/foxhound
    
  lib_detection:
    enable: true
    
    detection_timeout: 60 # 60
    post_cleanup: true  # cleanup lifted file or not    

vuln_db:
  connect: true 
  host: localhost
  port: 5430
  dbname: vulndb_annotated
  user: vulndb_annotated
  password: vulndb_pwd

# 3. static analysis configuration
staticpass:
  # timeout per vulnerability (analyze_hpg / run_traversals) in seconds
  poc_analysis_timeout: 1500 # 25 min
  # timeout for graph-based querying (analysis) of each site in seconds
  analysis_timeout: 3600 # 1 hr
  # enforce a max per webpage timeout for graph generation (in seconds)
  pagetimeout: 1200 # 20 min # only 1% got cutoff, should be fine
  # iteratively write the graph output to disk (useful in case of timeouts for partial results)
  iterativeoutput: false
  # max amount of available memory for static analysis per process
  memory: 64000
  # compress the property graph or not
  compress_hpg: true
  # overwrite the existing graphs or not
  overwrite_hpg: true
  # debug mode (disables isLibrary/isCdn skips, enables node --inspect)
  debug: false
  # timeout for neo4j container transactions in seconds (default: 300)
  container_transaction_timeout: 180 # 3 min, for tag tainting
  # neo4j instance config
  neo4j_user: neo4j
  neo4j_pass: root
  neo4j_http_port: '7474'
  # bolt port will default to http port + 2 with ineo
  # otherwise, specify another port here
  neo4j_bolt_port: '7476'
  neo4j_use_docker: true
  keep_docker_alive: false
  # Maximum number of matching nodes to process per code pattern (prevents performance issues)
  code_matching_cutoff: 50
  # Maximum recursion depth for taint propagation (prevents stack overflow)
  call_count_limit: 5


# 4. dynamic analysis configuration
dynamicpass:
  # time budget for dynamic analysis of each site in seconds
  sitetimeout: 10800 # 3 hrs
  # which browser to use
  browser:
    name: chrome
    # use remote browserstack browsers or not
    use_browserstack: false
    browserstack_username: xyz
    browserstack_password: xyz
    browserstack_access_key: xyz

# 5. verification pass
verificationpass:
  sitetimeout: 10800 # 3 hrs
  # which browser to use
  browser:
    name: chromeer
  endpoint: http://127.0.0.1:3456

cve_vuln:
  enabled: true
  passes:
    crawling: true
    lib_detection: true   
    vulndb: true
    static: true
    static_neo4j: true  
    lift: true  # enable lifting functionality
    transform: true  # enable sequence expression transformation
    # verification: false
    # crawling: false
    # static: true
    # static_neo4j: true
